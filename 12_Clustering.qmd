# Introduction to Clustering

Clustering is a data analysis technique whose objective is to group similar observations into clusters, such that:

- Individuals within the same group are as similar as possible (high internal homogeneity).

- Individuals from different groups are as different as possible (high external heterogeneity).

In other words, clustering seeks to discover hidden structures in the data without the need for prior labeling (unsupervised learning).

## Common Types of Clustering

1. Hierarchical Clustering: constructs a hierarchy of groups represented by a dendrogram. It can be:

- Agglomerative: each observation starts in its own cluster and they are progressively joined together.

- Divisive: it starts with a single cluster that is then divided.

2. **Partition-based clustering**: for example, K-means, which divides the data into a fixed number of clusters, optimizing the internal distance.

## Distance between individuals

The **distance between individuals** is a numerical measure of how different two observations are.

The most common are:

- Euclidean distance:
\[
d(x_i, x_j) = \sqrt{\sum_{k=1}^p (x_{ik} - x_{jk})^2}
\]

- Manhattan distance:

\[
d(x_i, x_j) = \sum_{k=1}^p |x_{ik} - x_{jk}|
\]

Where \(x_i, x_j\) are vectors of the variables of individuals \(i\) and \(j\).

## Distance between clusters

In hierarchical clustering, the **distance between clusters** is defined according to a **linkage method**, such as:

- **Single linkage**: minimum distance between elements of both clusters.

- **Complete linkage**: maximum distance between elements of both clusters.

- **Average linkage**: average of all pairwise distances.

- **Ward**: minimizes the total variance within each cluster.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(knitr)
library(VIM)   
library(ComplexHeatmap)
library(circlize)
library(cluster)
```

## 0. Data loading and creation of the `Total` object

```{r load-data}
anthropometrics <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS1_AnthropometricMeasurements/anthropometrics.csv")
vital_signs <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS6_VitalSigns/vital_signs.csv")
lifestyle <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS2_LifestyleHealth/lifestyle.csv")
biomarkers <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS4_Biomarkers/biomarkers.csv")
OSQ <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS8_SleepActivity/OviedoSleepQuestionnaire.csv")
IPAQ <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS7_PhysicalActivity/IPAQ.csv")


Total <- anthropometrics %>%
  full_join(vital_signs, by = c("id", "visit")) %>%
  full_join(lifestyle, by = c("id")) %>%
  full_join(biomarkers, by = c("id", "visit")) %>%
  full_join(OSQ, by = c("id", "visit")) %>%
  full_join(IPAQ, by = c("id", "visit"))

```


```{r}
# 1️⃣ Función para limpiar valores "<x"
clean_numeric <- function(x) {
  x <- trimws(x)  # quitar espacios
  x <- ifelse(grepl("^<", x),
              as.numeric(sub("<", "", x)) / 2,  # reemplazar <x por x/2
              x)
  suppressWarnings(as.numeric(x))  # convertir a numérico (NA si no se puede)
}

# 3️⃣ Detectar variables que deberían ser numéricas pero están como character
# 4️⃣ Excluir las columnas de identificación
# 3️⃣ Detectar columnas de texto que contienen números con "<"
char_vars <- names(Total)[sapply(Total, is.character)]
char_vars
```

```{r}
char_vars <- setdiff(char_vars, c("id", "visit", "defecation", "alcohol",
                                  "exercise", "categorical_score"))

# 5️⃣ Convertir solo las columnas problemáticas
Total <- Total %>%
  mutate(across(all_of(char_vars), clean_numeric))

# 5️⃣ Promediar por individuo y visita
Total_mean <- Total %>%
  group_by(id) %>%
  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "drop")
```


```{r, output=FALSE}
# 1️⃣ Separar variables de identificación
id_vars <- Total_mean %>% select(id, visit)
num_vars <- Total_mean %>% select(-id, -visit, -period, 
                                  -cigarettes_day, -cigars_day, -pipe_day,
                                  -others_psychological)

# 2️⃣ Aplicar imputación por kNN (por defecto k = 5)
set.seed(123)
num_imputed <- VIM::kNN(num_vars, k = 5, imp_var = FALSE)

# 3️⃣ Reconstruir el dataset completo
Total_imputed <- bind_cols(id_vars, num_imputed)

# 4️⃣ Revisar resultado
#summary(Total_imputed)
```


```{r}
num_scaled <- scale(num_imputed)
dim(num_scaled)
```


## 1. Distance matrix


We calculate Euclidean distances between individuals, which quantify how far apart they are in multidimensional space.


```{r correlacion, echo=TRUE}
dist_matrix <- as.matrix(dist(num_scaled, method = "euclidean"))

# Heatmap de las distancias
Heatmap(dist_matrix,
        name = "Distance",
        show_row_names = FALSE,
        show_column_names = TRUE,
        cluster_rows = TRUE,
        cluster_columns = TRUE,
        column_names_gp = gpar(fontsize = 8),
        col = colorRamp2(c(min(dist_matrix), mean(dist_matrix), max(dist_matrix)), 
                         c("blue", "white", "red")),
        heatmap_legend_param = list(title = "Distance"))

```

## Dendrograma

hclust performs hierarchical clustering, merging individuals step by step according to Ward's criterion, which minimizes the total within-cluster variance.

The dendrogram visualizes the merging process: the height of each merge indicates the distance between clusters.

```{r}
dist_matrix = dist(num_scaled, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma jerárquico (Ward.D2)")
```
```{r}
k <- 3
clusters <- cutree(hc, k = k)
table(clusters)
```
```{r}
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma con rectángulos (k=3)")
rect.hclust(hc, k = k, border = "red")
```
```{r}
sil <- silhouette(clusters, dist_matrix)
mean_sil <- mean(sil[, 3])
cat("Silhouette promedio (k=", k, "): ", round(mean_sil, 4), "\n", sep="")
```

```{r}
plot(sil, main = paste0("Silhouette (k=", k, ")"))
```


## Elbow's Graph

```{r}
kmax = 30
wss <- numeric(kmax) # dentro de suma de cuadrados
for (k in 1:kmax) {
km <- kmeans(num_scaled, centers = k, nstart = 25)
wss[k] <- km$tot.withinss
}

plot(1:kmax, wss, type = "b", pch = 19,
xlab = "Número de clusters (k)",
ylab = "Total within-cluster sum of squares (WSS)",
main = "Método del codo (Elbow Method)")
```

## K medias method

K-means is an unsupervised clustering algorithm widely used to partition a dataset into a predetermined number of groups, denoted by k. The method aims to assign each observation to one of the k clusters such that the within-cluster variation is minimized. More precisely, k-means minimizes the total within-cluster sum of squares, which measures how close the observations in each cluster are to the corresponding cluster centroid.

The algorithm proceeds iteratively in three main steps:

1. Initialization: Select k initial centroids, typically using random draws or algorithms such as k-means++.

2. Assignment step: Each observation is assigned to the nearest centroid according to a chosen distance metric, usually Euclidean distance.

3. Update step: New centroids are computed as the mean of all observations assigned to each cluster.

These two steps (assignment and update) are repeated until convergence, which occurs when cluster memberships no longer change or when the reduction in within-cluster variation becomes negligible.

Because the choice of k strongly influences the results, it is common practice to inspect different values of k and use diagnostic tools such as the elbow method or the silhouette coefficient to evaluate the quality of the clustering solution. K-means performs best when clusters are compact, roughly spherical, and have similar sizes, and when variables are properly scaled.

```{r}
set.seed(123)
km3 <- kmeans(num_scaled, centers = 3, nstart = 25)

km3$cluster
table(km3$cluster)
```

## Sillohuete graph

The *silhouette index* is a commonly used metric to evaluate the quality of a clustering solution. It measures how well each observation fits within its assigned cluster compared to other clusters.

For each observation *i*:

- \(a(i)\): the average distance between observation *i* and all other points in the same cluster (within-cluster cohesion).
- \(b(i)\): the smallest average distance between observation *i\) and all points in any other cluster (separation from the nearest alternative cluster).

The silhouette value for observation *i* is defined as:

\[
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}.
\]

The silhouette ranges from –1 to 1. Values close to **1** indicate that the observation is well matched to its cluster, values near **0** suggest overlapping clusters, and negative values indicate potential misclassification. The **average silhouette index** across all observations provides an overall measure of clustering quality and is commonly used to compare solutions with different numbers of clusters.


```{r}
dist_matrix <- dist(num_scaled)
sil_km <- silhouette(km3$cluster, dist_matrix)

mean_sil_km <- mean(sil_km[, 3])
cat("Silhouette promedio k-means (k=3): ", round(mean_sil_km, 4), "\n", sep="")
plot(sil_km, main = "Silhouette para k-means (k=3)")
```


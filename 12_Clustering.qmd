# Introduction to Clustering

Clustering is a data analysis technique whose objective is to group similar observations into clusters, such that:

- Individuals within the same group are as similar as possible (high internal homogeneity).

- Individuals from different groups are as different as possible (high external heterogeneity).

In other words, clustering seeks to discover hidden structures in the data without the need for prior labeling (unsupervised learning).

## Common Types of Clustering

1. Hierarchical Clustering: constructs a hierarchy of groups represented by a dendrogram. It can be:

- Agglomerative: each observation starts in its own cluster and they are progressively joined together.

- Divisive: it starts with a single cluster that is then divided.

2. **Partition-based clustering**: for example, K-means, which divides the data into a fixed number of clusters, optimizing the internal distance.

## Distance between individuals

The **distance between individuals** is a numerical measure of how different two observations are.

The most common are:

- Euclidean distance:
\[
d(x_i, x_j) = \sqrt{\sum_{k=1}^p (x_{ik} - x_{jk})^2}
\]

- Manhattan distance:

\[
d(x_i, x_j) = \sum_{k=1}^p |x_{ik} - x_{jk}|
\]

Where \(x_i, x_j\) are vectors of the variables of individuals \(i\) and \(j\).

## Distance between clusters

In hierarchical clustering, the **distance between clusters** is defined according to a **linkage method**, such as:

- **Single linkage**: minimum distance between elements of both clusters.

- **Complete linkage**: maximum distance between elements of both clusters.

- **Average linkage**: average of all pairwise distances.

- **Ward**: minimizes the total variance within each cluster.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(knitr)
library(VIM)   
library(ComplexHeatmap)
library(circlize)
library(cluster)
```

## 0. Data loading and creation of the `Total` object

```{r load-data}
anthropometrics <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS1_AnthropometricMeasurements/anthropometrics.csv")
vital_signs <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS6_VitalSigns/vital_signs.csv")
lifestyle <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS2_LifestyleHealth/lifestyle.csv")
biomarkers <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS4_Biomarkers/biomarkers.csv")
OSQ <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS8_SleepActivity/OviedoSleepQuestionnaire.csv")
IPAQ <- read_csv("C:/Users/UIB/Downloads/AI4FoodDB/DS7_PhysicalActivity/IPAQ.csv")


Total <- anthropometrics %>%
  full_join(vital_signs, by = c("id", "visit")) %>%
  full_join(lifestyle, by = c("id")) %>%
  full_join(biomarkers, by = c("id", "visit")) %>%
  full_join(OSQ, by = c("id", "visit")) %>%
  full_join(IPAQ, by = c("id", "visit"))

```


```{r}
# 1️⃣ Función para limpiar valores "<x"
clean_numeric <- function(x) {
  x <- trimws(x)  # quitar espacios
  x <- ifelse(grepl("^<", x),
              as.numeric(sub("<", "", x)) / 2,  # reemplazar <x por x/2
              x)
  suppressWarnings(as.numeric(x))  # convertir a numérico (NA si no se puede)
}

# 3️⃣ Detectar variables que deberían ser numéricas pero están como character
# 4️⃣ Excluir las columnas de identificación
# 3️⃣ Detectar columnas de texto que contienen números con "<"
char_vars <- names(Total)[sapply(Total, is.character)]
char_vars
```

```{r}
char_vars <- setdiff(char_vars, c("id", "visit", "defecation", "alcohol",
                                  "exercise", "categorical_score"))

# 5️⃣ Convertir solo las columnas problemáticas
Total <- Total %>%
  mutate(across(all_of(char_vars), clean_numeric))

# 5️⃣ Promediar por individuo y visita
Total_mean <- Total %>%
  group_by(id) %>%
  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "drop")
```


```{r, output=FALSE}
# 1️⃣ Separar variables de identificación
id_vars <- Total_mean %>% select(id, visit)
num_vars <- Total_mean %>% select(-id, -visit, -period, 
                                  -cigarettes_day, -cigars_day, -pipe_day,
                                  -others_psychological)

# 2️⃣ Aplicar imputación por kNN (por defecto k = 5)
set.seed(123)
num_imputed <- VIM::kNN(num_vars, k = 5, imp_var = FALSE)

# 3️⃣ Reconstruir el dataset completo
Total_imputed <- bind_cols(id_vars, num_imputed)

# 4️⃣ Revisar resultado
#summary(Total_imputed)
```


```{r}
num_scaled <- scale(num_imputed)
dim(num_scaled)
```


## 1. Distance matrix


We calculate Euclidean distances between individuals, which quantify how far apart they are in multidimensional space.


```{r correlacion, echo=TRUE}
dist_matrix <- as.matrix(dist(num_scaled, method = "euclidean"))

# Heatmap de las distancias
Heatmap(dist_matrix,
        name = "Distance",
        show_row_names = FALSE,
        show_column_names = TRUE,
        cluster_rows = TRUE,
        cluster_columns = TRUE,
        column_names_gp = gpar(fontsize = 8),
        col = colorRamp2(c(min(dist_matrix), mean(dist_matrix), max(dist_matrix)), 
                         c("blue", "white", "red")),
        heatmap_legend_param = list(title = "Distance"))

```

## Dendrograma

hclust performs hierarchical clustering, merging individuals step by step according to Ward's criterion, which minimizes the total within-cluster variance.

The dendrogram visualizes the merging process: the height of each merge indicates the distance between clusters.

```{r}
dist_matrix = dist(num_scaled, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma jerárquico (Ward.D2)")
```
```{r}
k <- 3
clusters <- cutree(hc, k = k)
table(clusters)
```
```{r}
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma con rectángulos (k=3)")
rect.hclust(hc, k = k, border = "red")
```
```{r}
sil <- silhouette(clusters, dist_matrix)
mean_sil <- mean(sil[, 3])
cat("Silhouette promedio (k=", k, "): ", round(mean_sil, 4), "\n", sep="")
```

```{r}
plot(sil, main = paste0("Silhouette (k=", k, ")"))
```

